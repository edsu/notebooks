{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Organizations",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCXl0a_VjYwU",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Organizations\n",
        "\n",
        "Imagine you have a list of organization names and you would like to know what their Twitter accounts are. This notebook explores how to look them up on Wikipedia, find the organizations home page, and then look on the homepage for a link to their Twitter account.\n",
        "\n",
        "\n",
        "## Get the Wikipedia Article\n",
        "\n",
        "First we will create a small function to get the Wikipedia Article for a given organization name. To do this we will install [requests_html](https://pypi.org/project/requests-html/) which is a nice Python library for doing HTTP requests and also for parsing returned JSON and HTML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz3wFTlpxBD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "3af8c883-b437-455f-f7cc-07bf78f42a9f"
      },
      "source": [
        "! pip install requests_html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting requests_html\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests_html) (0.0.1)\n",
            "Collecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/23/ea/ba7f9a5d728f7c45b298ae007b06908a5909eb5693a559ee5d70e7f84f92/parse-1.17.0.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests_html) (2.23.0)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests_html) (1.15.0)\n",
            "Collecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/41/79/3ec03b964c6e787ede1c34f29e5ead3f440965696ae0b3b0667f617deb75/pyee-7.0.3-py2.py3-none-any.whl\n",
            "Collecting urllib3<2.0.0,>=1.25.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 28.9MB/s \n",
            "\u001b[?25hCollecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s \n",
            "\u001b[?25hCollecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests_html) (2.10)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Building wheels for collected packages: parse, fake-useragent\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.17.0-cp36-none-any.whl size=24118 sha256=83182640608736893848a2075c03ad1ce2dd00dcad6a20898a7a12263336e5f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/ad/41/87f17d17fdc2b5f9648e2ec2f9dbe6ad51c2f58f086baafedf\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=60a4aec48ec3924ede8489b81b3b187e638fd6728878fa611836e3f218b0cf76\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built parse fake-useragent\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: parse, w3lib, appdirs, pyee, urllib3, tqdm, websockets, pyppeteer, cssselect, pyquery, fake-useragent, requests-html\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 fake-useragent-0.1.11 parse-1.17.0 pyee-7.0.3 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.48.2 urllib3-1.25.10 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxXlSZH6xNkV",
        "colab_type": "text"
      },
      "source": [
        "Now lets create a function which we can give the organization name, and which will query the Wikipedia's [Open Search API](https://www.mediawiki.org/wiki/API:Opensearch) to get a list of articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWLw7CwzDHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests_html\n",
        "\n",
        "http = requests_html.HTMLSession()\n",
        "\n",
        "def get_wikipedia_article(name):\n",
        "  url = 'https://en.wikipedia.org/w/api.php'\n",
        "  params = {\n",
        "      \"action\": \"opensearch\",\n",
        "      \"limit\": 1,\n",
        "      \"namespace\": 0,\n",
        "      \"format\": \"json\",\n",
        "      \"search\": name\n",
        "  }\n",
        "  results = http.get(url, params=params).json()\n",
        "  if len(results) == 4 and len(results[1]) > 0:\n",
        "    return results[3][0]\n",
        "  else:\n",
        "    return None  "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_rVgLNU1FRO",
        "colab_type": "text"
      },
      "source": [
        "Ok, lets test it out on *Exxon-Mobil*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H7-ZHkx0rxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c84815b-40b3-4c49-d5bc-e24d7d731217"
      },
      "source": [
        "get_wikipedia_article('Exxon-Mobil')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://en.wikipedia.org/wiki/ExxonMobil'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x0fuB7v0vu0",
        "colab_type": "text"
      },
      "source": [
        "Not bad! Lets try it with something that won't match just to make sure it doesn't throw an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5os9Vpj1Q9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_wikipedia_article('This is a fictitious organization name!')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE9lvesh1VKU",
        "colab_type": "text"
      },
      "source": [
        "## Get the Official Website"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ4pp0Ll1js9",
        "colab_type": "text"
      },
      "source": [
        "Maybe there's a way to get the official website data from the Wikipedia or Wikidata API. But it's pretty easy to get the HTML for the article and look for it since it appears in a standard way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCzNnq54vLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_official_website(article_url):\n",
        "  doc = http.get(article_url)\n",
        "  link = doc.html.find('.official-website .url a', first=True)\n",
        "  if link:\n",
        "    return link.attrs['href']\n",
        "  \n",
        "  # fall back to looking for the first \"Official website\" link\n",
        "  for a in doc.html.find('a'):\n",
        "    if re.match('official website', a.text, re.IGNORECASE):\n",
        "      return a.attrs['href']\n",
        "\n",
        "  return None"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItKAoixa8_bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a7fb6db-bf05-4c2d-c283-5737d4caacc6"
      },
      "source": [
        "get_official_website('https://en.wikipedia.org/wiki/ExxonMobil')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://www.exxonmobil.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK8ZCmPQ9C4J",
        "colab_type": "text"
      },
      "source": [
        "Nice!\n",
        "\n",
        "## Twitter Account\n",
        "\n",
        "Now that we know the homepage for the organization we need a function that can look for a link to their Twitter account.\n",
        "\n",
        "This is a bit more complicated because we don't know where on the page the link will be or even if there are multiple links. We do know that twitter account URLs will look something like:\n",
        "\n",
        "    https://twitter.com/{account-name}\n",
        "\n",
        "Where *account-name* can be any sequence of letters, numbers and underscores. Twitter used to allow you to link with a hashbang URL, which is worth looking for too eventhough it is deprecated:\n",
        "\n",
        "    https://twitter.com/#!/{account-name}\n",
        "\n",
        "So we can look at all the links in the page that match that pattern and return the account that has the most links."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGPlY2U-9hbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_twitter(url):\n",
        "\n",
        "  # count all the accounts\n",
        "  accounts = Counter()\n",
        "\n",
        "  doc = http.get(url)\n",
        "  for a in doc.html.find('a[href]'):\n",
        "    m = re.match(r'.*twitter.com/(#!/)?([a-z0-9_]+).*', a.attrs['href'], re.IGNORECASE)\n",
        "    if m:\n",
        "      accounts[m.group(2)] += 1\n",
        "\n",
        "  if len(accounts) > 0:\n",
        "    return '@' + accounts.most_common()[0][0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdpj2DlFB9lL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "674e65b3-a560-4ea8-ab52-b317329c1c54"
      },
      "source": [
        "get_twitter('http://www.exxonmobil.com')"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'@exxonmobil'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XbvAlYrCCs-",
        "colab_type": "text"
      },
      "source": [
        "## Putting It Together\n",
        "\n",
        "Now for the fun part since we can create a function that orchestrates these three functions into one that takes an organization name and returns the Twitter handle for it.\n",
        "\n",
        "Note: we do need to take care that each step in the process returns a result in order to keep going."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_KZdhpIDL51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_info(org_name):\n",
        "\n",
        "  article = get_wikipedia_article(org_name)\n",
        "\n",
        "  if article:\n",
        "    homepage = get_official_website(article)\n",
        "  else:\n",
        "    homepage = None\n",
        "\n",
        "  if homepage:\n",
        "    twitter = get_twitter(homepage)\n",
        "  else:\n",
        "    twitter = None\n",
        "\n",
        "  return {\n",
        "      \"article\": article,\n",
        "      \"homepage\": homepage,\n",
        "      \"twitter\": twitter\n",
        "  }\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGIm7T_7ESeQ",
        "colab_type": "text"
      },
      "source": [
        "Let's try it out! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTG8lqijE0rT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cf14fb8d-e9cc-4600-9c1b-a446e4b03d64"
      },
      "source": [
        "get_info('Exxon Mobil')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/ExxonMobil',\n",
              " 'homepage': 'http://www.exxonmobil.com',\n",
              " 'twitter': '@exxonmobil'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFwEjyj3FLGr",
        "colab_type": "text"
      },
      "source": [
        "🎉 Let's try it with some other organizations now. Maybe it only works for Exxon-Mobil..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvt9byiIFcMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d1474fbd-b666-495d-b503-f9724659d9c3"
      },
      "source": [
        "get_info('Sears Roebuck')"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/Sears_Roebuck',\n",
              " 'homepage': 'http://www.sears.com',\n",
              " 'twitter': '@searsdeals'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2NjC9jNFfen",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4e298c5e-0e57-432f-e9b1-bbba469ecb9b"
      },
      "source": [
        "get_info('US Navy')"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/US_Navy',\n",
              " 'homepage': 'http://www.navy.mil/',\n",
              " 'twitter': '@usnavy'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC42P9jdIfQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "7f2419cd-28d4-46d2-8899-d7e1071464ed"
      },
      "source": [
        "get_info(\"McDonalds\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/McDonald%27s',\n",
              " 'homepage': 'https://www.mcdonalds.com',\n",
              " 'twitter': '@McDonalds'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z1kMY51IiEh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9bf61812-9782-4842-da98-141fb158be07"
      },
      "source": [
        "get_info('University of Maryland')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/University_of_Maryland,_College_Park',\n",
              " 'homepage': 'https://www.umd.edu/',\n",
              " 'twitter': '@UofMaryland'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0TYA0HfShQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6aeda121-7e5f-4c6d-dbbe-dbf15d134af2"
      },
      "source": [
        "get_info('Chicago Police Department')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'article': 'https://en.wikipedia.org/wiki/Chicago_Police_Department',\n",
              " 'homepage': 'https://home.chicagopolice.org/',\n",
              " 'twitter': '@Chicago_Police'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIpJiN2rSlQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}