{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArchiveTeam Ingest Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to chart the growth of the ArchiveTeam collection at the Internet Archivee over time. It relies on the [Internet Archive's Scrape API](https://archive.org/help/aboutsearch.htm) to iterate through all the items in a collections.\n",
    "\n",
    "Once we have summarize the item metadata for the ArchiveTeam's collection we'll create some simple charts to visualize the growth over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Item Metadata\n",
    "\n",
    "We're going to start with the most complicated piece first. We need a function to return all the item metadata from the API. In addition to the *collection_id* to get items for we'll also allow it to take a *count* parameter to limit the number of results to return (which is useful in testing). We'll also add an optional progress bar for long runnind data collection, so you know that something is going on.\n",
    "\n",
    "The *get_items* function returns an interator which we will use in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import requests\n",
    "\n",
    "def get_items(collection_id, count=None, progress=False):        \n",
    "    url = \"https://archive.org/services/search/v1/scrape\"\n",
    "    params = {\n",
    "        \"q\": \"collection:\" + collection_id,\n",
    "        \"fields\": \"collection,item_size,addeddate,identifier\",\n",
    "        \"sorts\": \"addeddate desc\",\n",
    "        \"rows\": 1000\n",
    "    }\n",
    "    \n",
    "    items_seen = 0\n",
    "    cursor = None\n",
    "    stop = False\n",
    "    \n",
    "    # set up a progress bar for feedback when collecting a lot of data\n",
    "    if progress and not count:\n",
    "        results = requests.get(url, params=params).json()\n",
    "        progress_bar = tqdm.tqdm(total=results['total'], unit='item')\n",
    "    elif progress:\n",
    "        progress_bar = tqdm.tqdm(total=count, unit='item')\n",
    "    else:\n",
    "        progress_bar = None\n",
    "    \n",
    "    while not stop:\n",
    "        results = requests.get(url, params=params).json()\n",
    "        if results['count'] == 0:\n",
    "            stop = True\n",
    "        else:\n",
    "            for item in results['items']:\n",
    "                items_seen += 1\n",
    "                if progress_bar:\n",
    "                    progress_bar.update(1)\n",
    "                if count and items_seen > count:\n",
    "                    stop = True\n",
    "                    break\n",
    "                yield item\n",
    "            \n",
    "            if 'cursor' not in results:\n",
    "                stop = True\n",
    "            else:\n",
    "                params['cursor'] = results['cursor']            \n",
    "\n",
    "    if progress_bar:\n",
    "        progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the function to get 10 results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identifier': 'chromebot-2019-01-02-c6334f', 'addeddate': '2019-01-02T19:21:22Z', 'collection': ['archiveteam_chromebot', 'archiveteam'], 'item_size': 95295847}\n",
      "{'identifier': 'archiveteam_archivebot_go_20190102180002', 'addeddate': '2019-01-02T18:17:48Z', 'collection': ['archivebot', 'archiveteam'], 'item_size': 48535883274}\n",
      "{'identifier': 'archiveteam_newssites_20190102170029', 'addeddate': '2019-01-02T18:00:12Z', 'collection': ['archiveteam_newssites', 'archiveteam'], 'item_size': 55466926841}\n",
      "{'identifier': 'archiveteam_newssites_20190102155155', 'addeddate': '2019-01-02T16:59:36Z', 'collection': ['archiveteam_newssites', 'archiveteam'], 'item_size': 55418539339}\n",
      "{'identifier': 'youtube-H6B7HmoeGGM', 'addeddate': '2019-01-02T16:58:36Z', 'collection': ['archiveteam_youtube', 'archiveteam'], 'item_size': 306371916}\n",
      "{'identifier': 'youtube-3UIRB6H7Gh8', 'addeddate': '2019-01-02T16:42:24Z', 'collection': ['archiveteam_youtube', 'archiveteam'], 'item_size': 40638932}\n",
      "{'identifier': 'youtube-aIzFKRMDQHc', 'addeddate': '2019-01-02T16:42:24Z', 'collection': ['archiveteam_youtube', 'archiveteam'], 'item_size': 14210043}\n",
      "{'identifier': 'youtube-qgInvturSbE', 'addeddate': '2019-01-02T16:42:24Z', 'collection': ['archiveteam_youtube', 'archiveteam'], 'item_size': 45044739}\n",
      "{'identifier': 'youtube-JEVALjFgalU', 'addeddate': '2019-01-02T16:28:27Z', 'collection': ['archiveteam_youtube', 'archiveteam'], 'item_size': 21415668}\n",
      "{'identifier': 'archiveteam_newssites_20190102150823', 'addeddate': '2019-01-02T16:09:50Z', 'collection': ['archiveteam_newssites', 'archiveteam'], 'item_size': 58732037975}\n"
     ]
    }
   ],
   "source": [
    "for item in get_items('archiveteam', count=10):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "Now we can write a function that will walk over a set of items and summarize some things like the sizes by day, and sizes by collection. The results are returned as Counter objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def summarize(items):\n",
    "    bytes_per_day = Counter()\n",
    "    bytes_per_collection = Counter()\n",
    "    \n",
    "    for item in items:\n",
    "        date = item.get('addeddate', '').split('T')[0]\n",
    "        if date:\n",
    "            bytes_per_day[date] += item['item_size']\n",
    "        for coll_id in item['collection']:\n",
    "            bytes_per_collection[coll_id] += item['item_size']\n",
    "            \n",
    "    return {\n",
    "        \"bytes_per_day\": bytes_per_day,\n",
    "        \"bytes_per_collection\": bytes_per_collection\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this out on 10,000 of the most recent items uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(get_items('archiveteam', count=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the three highest upload days for those items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2018-12-12', 6136367854556),\n",
      " ('2018-12-11', 5653343238291),\n",
      " ('2018-12-17', 5605217081034)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(summary['bytes_per_day'].most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the top fivee collections by size for those items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('archiveteam', 82220767031003),\n",
      " ('archiveteam_tumblr', 46294872028478),\n",
      " ('archivebot', 29007696205866),\n",
      " ('archiveteam_newssites', 4001579842223),\n",
      " ('archiveteam_youtube', 2011535245638)]\n"
     ]
    }
   ],
   "source": [
    "pprint(summary['bytes_per_collection'].most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect\n",
    "\n",
    "Now we're ready to run through all the results. It's going to take a while for ArchiveTeam since they have about half a million items. So notice we turn on the progress bar so you can see that something is happening. Also let's only do this full summariation when we don't already have the CSV files that we're going to save the data in.\n",
    "\n",
    "Are you ready? Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 205001/494892 [19:11<20:23, 237.02item/s]   "
     ]
    }
   ],
   "source": [
    "from csv import DictWriter\n",
    "\n",
    "if True or not os.path.isfile('ArchiveTeam-sizes.csv'):\n",
    "    \n",
    "    summary = summarize(get_items('archiveteam', progress=True))\n",
    "          \n",
    "    with open('ArchiveTeam-collections.csv', 'w') as output:\n",
    "        writer = DictWriter(output, fieldnames=['collection', 'size'])\n",
    "        writer.writeheader()\n",
    "        for collection, size in summary['bytes_per_collection'].items():\n",
    "            writer.writerow({\"collection\": collection, \"size\": size})\n",
    "            \n",
    "    with open('ArchiveTeam-dates.csv', 'w') as output:\n",
    "        writer = DictWriter(output, fieldnames=['date', 'size'])\n",
    "        writer.writeheader()\n",
    "        # note: we sort the csv by date\n",
    "        for date, size in sorted(summary['bytes_per_day'].items()):\n",
    "            writer.writerow({\"date\": date, \"size\": size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze\n",
    "\n",
    "So now we have our summary data in CSV files, so we can (finally) analyze it a bit with [pandas](https://pandas.pydata.org/) and maybe generate a useful graph or two.\n",
    "\n",
    "First we'll import pandas and load in the dates summary data as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sizes = pd.read_csv('ArchiveTeam-dates.csv', index_col=0, parse_dates=True)\n",
    "sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we've got a DataFrame that is indexed by the day, with one Series *size* that contains the bytes. I don't know about you, but I find it difficult to think of size in terms of bytes. So let's use pandas to calcuate a gigabyte column for us using the bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = sizes.assign(gb=lambda x: x / 1024 ** 3)\n",
    "sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tell pandas to calcuate some statistics on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Since we have thousands of days, it might be useful to see the stats by month rather than by day. That's not too hard to do since our dataframe as a date index and pandas support for [timeseries](https://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries) data allows us to resample the dataframe on a monthly basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_by_month = sizes.resample('M').sum()\n",
    "sizes_by_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plot = sizes_by_month['gb'].plot(figsize=(15, 5))\n",
    "\n",
    "plot.set_xlabel('Year')\n",
    "plot.set_ylabel('Gigabytes per Month')\n",
    "plot.set_title('ArchiveTeam Ingest Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colls = pd.read_csv('ArchiveTeam-collections.csv', index_col=0, parse_dates=True)\n",
    "colls = colls.assign(gb=lambda x: x / 1024 ** 3)\n",
    "colls = colls.sort_values(by='gb', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colls = colls.drop('university_maryland_cp')\n",
    "favs = filter(lambda i: i.startswith('fav-'), colls.index)\n",
    "colls.drop(labels=favs, inplace=True)\n",
    "plot = colls.plot.pie('gb', figsize=(15, 20), legend=False, title=\"Gigabytes per Collection\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
