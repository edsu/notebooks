{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArchiveTeam Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook lets you calculate the growth of the data deposited at the Internet Archive by ArchiveeTeam. It just uses the Internet Archive API to look through the metadata for all their items.\n",
    "\n",
    "## Install\n",
    "\n",
    "To get started you'll need to save your Internet Archive account login details. If you don't have an account go over to archive.org and create one, and then:\n",
    "\n",
    "    % ia configure\n",
    "    \n",
    "## Internet Archive Metadata\n",
    "\n",
    "Now we're ready to load and use the [internetarchive](https://github.com/jjjake/internetarchive) Python extension:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet Archive organizes their stuff according to *collections*, which can contain one or many *items*. Each item then can contain one or many files. Collections and items have identifiers that uniquely identify them. For example if you know the the item identifier `ARCHIVEIT-2410-DAILY-JOB227586-20160728-00000` you can go get it and print its metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'addeddate': '2018-12-21 19:03:50',\n",
      " 'collection': ['archiveteam_tumblr', 'archiveteam'],\n",
      " 'date': '2018-12',\n",
      " 'identifier': 'archiveteam_tumblr20181221175524',\n",
      " 'language': 'eng',\n",
      " 'mediatype': 'web',\n",
      " 'publicdate': '2018-12-21 19:03:50',\n",
      " 'title': 'Archive Team Tumblr Tumbledown: 20181221175524',\n",
      " 'uploader': 'me@harrycross.me'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "item = ia.get_item('archiveteam_tumblr20181221175524')\n",
    "pprint(item.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the item identifier `archiveteam_tumblr20181221175524` contains a date/time? We're going to take a bit of a leap here and assume that the date contained in there is the date that the WARC data was collected from SavePageNow. This might in fact not be the case, unless we learn more from Internet Archive about the provenance of this data.\n",
    "\n",
    "There's actually much more detailed metadata available for the files in the item. Here's how many files are in the item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(item.item_metadata['files']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what metadata is available for the first file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'btih': 'fb9192f71567e55112fdcc8bbec9961431a94d21',\n",
      " 'crc32': '988d5423',\n",
      " 'format': 'Archive BitTorrent',\n",
      " 'md5': 'd5545927ddd7f87e06d5985553c568f3',\n",
      " 'mtime': '1545421126',\n",
      " 'name': 'archiveteam_tumblr20181221175524_archive.torrent',\n",
      " 'sha1': '08b02a02f9c601d0d7d42a2adc409f2a5197e422',\n",
      " 'size': '258891',\n",
      " 'source': 'metadata'}\n"
     ]
    }
   ],
   "source": [
    "pprint(item.item_metadata['files'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the *size* property? That's the size in bytes of the file.\n",
    "\n",
    "## Fetch the Data\n",
    "\n",
    "So now we know enough to write a function that can return the date and the size of the warc files in an item given its identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def item_summary(item_id):\n",
    "    item = ia.get_item(item_id)\n",
    "\n",
    "    size = 0\n",
    "    for file in item.item_metadata['files']:\n",
    "        if 'size' in file:\n",
    "            size += int(file['size'])\n",
    "            \n",
    "    m = re.match('(^\\d\\d\\d\\d-\\d\\d-\\d\\d)', item.item_metadata['metadata']['addeddate'])\n",
    "    date = m.group(1)\n",
    "    \n",
    "    return date, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2018-12-21', 53766825137)\n"
     ]
    }
   ],
   "source": [
    "print(item_summary('archiveteam_tumblr20181221175524'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internetarchive Python library doesn't offer an abstraction for collections. But it does provide a way to search for a collection and iterate through the items that it contains. If you now the name of your collection you can iterate through it pretty easily. So lets see how many items are contained in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493237\n"
     ]
    }
   ],
   "source": [
    "num_items = len(ia.search_items('collection:archiveteam'))\n",
    "print(num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's quite a few. If it takes a second to get the metadata for each item we are going to need to wait a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.0102777777778 hours\n"
     ]
    }
   ],
   "source": [
    "print(num_items / 60 / 60.0, 'hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so let's go through each item, get the size and day for the item, and store them in a dictionary. Since there may be more than one item in a day it's important to add to the existing value for a date instead of simply storing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "sizes = {}\n",
    "\n",
    "if not os.path.isfile('ArchiveTeam.csv'):\n",
    "\n",
    "    for result in ia.search_items('collection:archiveteam'):\n",
    "        date, size = item_summary(result['identifier'])\n",
    "        sizes[date] = sizes.get(date, 0) + size\n",
    "        print(result['identifier'], date, size, sizes[date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write out the *sizes* dictionary to a CSV file, where every row is a date. This way we won't need to fetch it every time we run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sizes:\n",
    "\n",
    "    import csv\n",
    "\n",
    "    dates = sorted(sizes.keys())\n",
    "\n",
    "    with open('ArchiveTeam.csv', 'w') as output:\n",
    "        writer = csv.writer(output)\n",
    "        writer.writerow(['date', 'size'])\n",
    "        for date in dates:\n",
    "            writer.writerow([date, sizes[date]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Data\n",
    "\n",
    "So now we have our CSV of data we can analyze it a bit with [pandas](https://pandas.pydata.org/) and maybe generate a useful graph. First we'll import pandas and load in the data as a pandas DataFrame.\n",
    "\n",
    "*Aside: I'm still learning pandas, and this is not meant to be a tutorial. If you want to learn more about all the amazing stuff you can do with it you'll want to spend some time in their excellent [tutorial](https://pandas.pydata.org/pandas-docs/stable/dsintro.html). If you work with R at all it should look pretty familiar. If not, treat this as just dipping your toe in to test the water. That's what I'm doing. If you do know pandas and notice a better way of doing any of this please let me know!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sizes = pd.read_csv('ArchiveTeam.csv', index_col=0, parse_dates=True)\n",
    "sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the liveweb data started saving back in 2011. So now we've got a DataFrame that is indexed by the day, with one Series *size* that contains the bytes. I don't know about you, but I find it difficult to think of size in terms of bytes. So let's use pandas to calcuate a gigabyte column for us using the bytes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = sizes.assign(gb=lambda x: x / 1024 ** 3)\n",
    "sizes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tell pandas to calcuate some statistics on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Since we have thousands of days, it might be useful to see the stats by month rather than by day. That's not too hard to do since our dataframe as a date index and pandas support for [timeseries](https://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries) data allows us to resample the dataframe on a monthly basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes_by_month = sizes.resample('M').sum()\n",
    "sizes_by_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sizes_by_month['gb'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinda cool right!? The dip at the end is the result of me running the data collection at the beginning of November. So let's remove that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "sizes_without_nov = sizes_by_month['gb'].drop(datetime.date(2018, 11, 30))\n",
    "\n",
    "plot = sizes_without_nov\n",
    ".plot(figsize=(12, 5))\n",
    "plot.set_xlabel('Year')\n",
    "plot.set_ylabel('Gigabytes per Month')\n",
    "plot.set_title('Save-Page-Now Ingest Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
